{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specter Embeddings\n",
    "\n",
    "Delvin So\n",
    "\n",
    "This notebook\n",
    "\n",
    "1) performs minimal pre-processing to get the datasets into a SPECTER inference ready format\n",
    "\n",
    "2) uses the provided bash script in the SPECTER repo to perform inference, embedding the abstracts\n",
    "\n",
    "3) cleans the `jsonl` file from SPECTER into a pickle for downstream modeling\n",
    "\n",
    "\n",
    "NOTE: This is likely much easier to implement now with HF support for specter!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow instructions for setting up specter per: https://github.com/allenai/specter\n",
    "\n",
    "Then, download datasets_complete and unzip into `specter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = '.'\n",
    "os.chdir(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following prepares the datasets for input into specter, namely creating a list of paper ids and a json with the paper id, abstract and titles of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../cleaned_data/*tsv'\n",
    "out_dir = '../specter/datasets_json'\n",
    "\n",
    "if not os.path.exists(out_dir): os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = glob(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n"
     ]
    }
   ],
   "source": [
    "for fn in f: \n",
    "    basen = os.path.basename(fn)\n",
    "    txt = os.path.splitext(basen)[0]\n",
    "    txt = txt.split('_')[0].replace(' ', '_')\n",
    "\n",
    "    print(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../cleaned_data/sample_oct.tsv\n",
      "../specter/datasets_json/sample.json\n",
      "\tBefore 200\n",
      "NULl TITlES:0\n",
      "\tAfter 200\n",
      "Dataset: sample Size: 200\n",
      "\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "length_d = {\n",
    "    \n",
    "    'ADIPP': 47491,\n",
    "    'BBD': 43569,\n",
    "    'Hydronephrosis': 4339,\n",
    "    'NCDs': 17533,\n",
    "    'Rehab': 12042,\n",
    "    'Scaling': 10679,\n",
    "    'VitaminD': 1397,\n",
    "    'WASH': 5986,\n",
    "    'mucosal': 3987,\n",
    "    'pibd' : 7560,\n",
    "    'ocd' : 347,\n",
    "    'sample': 200\n",
    "}\n",
    "\n",
    "\n",
    "for fn in f:\n",
    "    # ----- construct new filename for json and ids ----\n",
    "    print(fn)\n",
    "    basen = os.path.basename(fn)\n",
    "    txt = os.path.splitext(basen)[0]\n",
    "    txt = txt.split('_')[0].replace(' ', '_')\n",
    "    \n",
    "    out_json = os.path.join(out_dir, '{}.json'.format(txt))\n",
    "    out_ids = os.path.join(out_dir, '{}.ids'.format(txt))\n",
    "    out_labels_ids = os.path.join(out_dir, '{}_labels_ids.tsv'.format(txt))\n",
    "    print(out_json)\n",
    "    \n",
    "    # ------ read in each file and ultimately dump to json per specter formatting -----\n",
    "    if fn.endswith('tsv'):\n",
    "\n",
    "        dat = pd.read_csv(fn, encoding = 'ISO-8859-1', sep = '\\t')\n",
    "        \n",
    "    elif fn.endswith('xlsx'):\n",
    "        \n",
    "        dat = pd.read_excel(fn)\n",
    "        \n",
    "        \n",
    "\n",
    "    # ----- processing identically to our datasets -----\n",
    "    # TODO: if we have time before manuscript, fix this\n",
    "    \n",
    "    print('\\tBefore ' + str(dat.shape[0]))\n",
    "\n",
    "    # Removes the filler ' ' from the first notebook. Could go immediately from ' ' to None but useful as a sanity check.\n",
    "    dat['Title'] = dat['Title'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    print('NULl TITlES:' + str(dat['Title'].isnull().sum()))\n",
    "    dat['Title'] = dat['Title'].replace(np.nan, 'None') \n",
    "    \n",
    "    print('\\tAfter ' + str(dat.shape[0]))\n",
    "\n",
    "\n",
    "    # ----- processing for specter -----\n",
    "    \n",
    "\n",
    "    print('Dataset: {} Size: {}'.format(txt, dat.shape[0]))\n",
    "    assert(np.array(dat.index).shape[0] == length_d[txt]), 'Dataset sizes do not line up!!'\n",
    "\n",
    "    dat.columns = map(str.lower, dat.columns) # lowercase for compatability w/ specter\n",
    "    dat = dat.rename(columns = {'unq_id':'paper_id'}) # paper_id is the identifier for specter inference\n",
    "    \n",
    "    # ----- just json things -----\n",
    "    dat.index = dat.paper_id # copy unique id over to the index column so it matches up with the json row #, for inference\n",
    "    new_json = dat[['title', 'abstract', 'paper_id']].to_json(orient = 'index')\n",
    "    parsed_json = json.loads(new_json)\n",
    "\n",
    "    with open(out_json, 'w', encoding = 'utf-8') as f:\n",
    "        json.dump(parsed_json, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    # ---- create the '*.ids' file as needed for inference ----\n",
    "    \n",
    "    dat.paper_id.to_csv(out_ids, header=False, index = False)\n",
    "    \n",
    "    # ---- create the labels and ids to map the embeddings back to -----\n",
    "    # quoting accounts for delimiters such as \\r found in 'mucosal'\n",
    "    dat.to_csv(out_labels_ids, index = False, sep = '\\t', quoting=csv.QUOTE_NONNUMERIC) \n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check to see \"None\" exists in the title and nothing else was replaced by accident\n",
    "for f in glob('datasets_json/*labels_ids*'):\n",
    "    test = pd.read_csv(f, sep = '\\t')\n",
    "    print(f + '\\n\\t')\n",
    "    print(test[test.title.str.contains(r'\\bNone\\b', regex=True, case=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick and dirty bash script to loop through each dataset and embed, saving to `out_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# make sure you conda activate specter, are in the specter dir, and\n",
    "# the ids and output are absolute as specter doesn't like relative paths\n",
    "data_dir=/home/delvinso/sra_sample/data_preprocessing/specter\n",
    "out_dir=/home/delvinso/sra_sample/data_preprocessing/specter/embed_json\n",
    "mkdir -p ${out_dir}\n",
    "\n",
    "for f in $(ls  ${data_dir}/datasets_json/*ids | egrep -v 'ADIPP|Scaling|Rehab|mucosal')\n",
    "#for f in $(ls  ${data_dir}/datasets_json/*ids | egrep 'ADIPP|Scaling|Rehab|mucosal|ocd')\n",
    "do\n",
    "    f=$(basename ${f} | sed 's/\\.ids//g')\n",
    "    echo ${f}\n",
    "    \n",
    "    \n",
    "    python3 scripts/embed.py \\\n",
    "        --ids ${data_dir}/datasets_json/${f}.ids --metadata ${data_dir}/datasets_json/${f}.json \\\n",
    "        --model ./model.tar.gz \\\n",
    "        --output-file ${out_dir}/${f}.jsonl \\\n",
    "        --vocab-dir data/vocab/ \\\n",
    "        --batch-size 58 \\\n",
    "        --cuda-device 0\n",
    "\n",
    "done\n",
    "echo \"Done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specter Embedding Cleaning\n",
    "\n",
    "The following code creates a dictionary of abstract IDs, embedding, and target (abstract inclusion) for input into our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../specter/embed_json/sample.jsonl']\n"
     ]
    }
   ],
   "source": [
    "embed_jsons = glob(os.path.join('..', 'specter', 'embed_json', '*jsonl'))\n",
    "\n",
    "print(embed_jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@galea/how-to-love-jsonl-using-json-line-format-in-your-workflow-b6884f65175b\n",
    "def load_jsonl(input_path) -> list:\n",
    "    \"\"\"\n",
    "    Read list of objects from a JSON lines file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "    print('Loaded {} records from {}'.format(len(data), input_path))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "Loaded 200 records from ../specter/embed_json/sample.jsonl\n",
      "Index(['paper_id', 'all_text_clean', 'metadata_clean', 'inclusion',\n",
      "       'fulltext_inclusion', 'covidence..', 'title', 'abstract', 'all_text',\n",
      "       'metadata'],\n",
      "      dtype='object')\n",
      "Dataset: sample Size: 200\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out_pickle = os.path.join('..', '..', 'pickles', 'specter')\n",
    "\n",
    "# dictionary for comparing dataset sizes \n",
    "length_d = {\n",
    "    \n",
    "    'ADIPP': 47491,\n",
    "    'BBD': 43569,\n",
    "    'Hydronephrosis': 4339,\n",
    "    'NCDs': 17533,\n",
    "    'Rehab': 12042,\n",
    "    'Scaling': 10679,\n",
    "    'VitaminD': 1397,\n",
    "    'WASH': 5986,\n",
    "    'mucosal': 3987,\n",
    "    'pibd' : 7560,\n",
    "    'ocd': 347,\n",
    "    'sample': 200\n",
    "}\n",
    "\n",
    "if not os.path.exists(out_pickle): os.makedirs(out_pickle)\n",
    "    \n",
    "for f in embed_jsons:\n",
    "    \n",
    "    bn = os.path.basename(f).split('.')[0]\n",
    "    out_fn = os.path.join(out_pickle, bn + '.p')\n",
    "    print(bn)\n",
    "    \n",
    "    # ---- load in the specter embeddings (json) and convert to dataframe -----\n",
    "    jsonl = load_jsonl(f)\n",
    "    jsonl_df = pd.DataFrame(jsonl)\n",
    "\n",
    "\n",
    "    # ----- read in the data used to created the specter embeddings ------\n",
    "    og_dat = pd.read_csv(os.path.join('..', 'specter', 'datasets_json/' + bn + '_labels_ids.tsv'), sep = '\\t' )\n",
    "\n",
    "    print(og_dat.columns)\n",
    "\n",
    "    try:\n",
    "        og_dat = og_dat.rename(columns = {'covidence..': 'covidence #'})\n",
    "    except e:\n",
    "        print('Covidence.. not found in columns, skipped')\n",
    "\n",
    "    # ----- get labels and join to embeddings ------\n",
    "    \n",
    "    try:\n",
    "        jsonl_df2 = jsonl_df.set_index('paper_id')\\\n",
    "            .join(og_dat[['paper_id', 'inclusion', 'fulltext_inclusion', 'covidence #']]\\\n",
    "            .set_index('paper_id'), how = 'left')\n",
    "    except KeyError:\n",
    "        jsonl_df2 = jsonl_df.set_index('paper_id')\\\n",
    "            .join(og_dat[['paper_id', 'inclusion', 'covidence #']]\\\n",
    "            .set_index('paper_id'), how = 'left')\n",
    "\n",
    "    jsonl_df2 = jsonl_df2.reset_index()  # turns index, or paper id back into a column\n",
    "\n",
    "    # ----- validate -----\n",
    "    print('Dataset: {} Size: {}'.format(bn, np.array(jsonl_df2.paper_id).shape[0]))\n",
    "    assert(np.array(jsonl_df2.paper_id).shape[0] == length_d[bn]), 'Dataset size does not match known ones!!'\n",
    "\n",
    "\n",
    "    # ----- create the dictionary of embeddings -----\n",
    "    d = {}\n",
    "    d['ids'] = np.array(jsonl_df2.paper_id)\n",
    "    d['embeddings'] = np.vstack(jsonl_df2.embedding)\n",
    "#     print(d['embeddings'][0].shape)\n",
    "    d['labels'] = np.concatenate(np.vstack(jsonl_df2.inclusion))\n",
    "    try:\n",
    "        d['final_labels'] = np.concatenate(np.vstack(jsonl_df2.fulltext_inclusion))\n",
    "    except AttributeError:\n",
    "        d['final_labels'] = None\n",
    "    d['title'] = np.concatenate(np.vstack(jsonl_df2.title))\n",
    "    d['covidence #'] = np.concatenate(np.vstack(jsonl_df2['covidence #']))\n",
    "\n",
    "    pickle.dump(d, open(out_fn, 'wb'))\n",
    "    \n",
    "print('Done!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
