{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Systematic Review Dataset Pre-Processing and Basic Stats\n",
    "\n",
    "Delvin So & Chantal Shaib\n",
    "\n",
    "Current iteration:\n",
    "- normalizes the datasets so each one has a unique identifier and requires no more additional pre-processing, at least for input into our current experiment scripts. \n",
    "- This also makes the dataset almost immediately SPECTER compliant (ie. minimal pre-processing but nothing that requires altering rows, but even then there is now a unique ID so whatever).\n",
    "- fixes a mistake where 'empty' abstracts were not actually empty and kept in the dataset(s)\n",
    "\n",
    "Previously there was no unique identifier and all other information was dropped, making it difficult to keep track of the data whenever any additional pre-processing was performed (which was required internally within our model and in SPECTER), as well as for any ad-hoc analyses. \n",
    "\n",
    "* requires some changes in `utils.py` and `AbstractDataset.py` - TODO: generalize the column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import csv\n",
    "from glob import glob\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, regexp, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: please run your `jupyter-lab / notebook` in the *root* (`sra`) directory!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/delvin/Downloads/sra_sample/data_preprocessing/notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/delvin/Downloads/sra_sample/data_preprocessing\n"
     ]
    }
   ],
   "source": [
    "# set to root directory\n",
    "os.chdir('..')\n",
    "print(os.getcwd())\n",
    "\n",
    "output_dir = os.path.join('..', 'cleaned_data')\n",
    "data_dir = os.path.join('data', 'datasets_complete')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below needs to be applied to the datasets prior to parsing through specter\n",
    "def preprocess_df(dataset: pd.DataFrame):\n",
    "    \n",
    "       \n",
    "    print('\\tBefore ' + str(dataset.shape[0]))\n",
    "\n",
    "    # we want to drop on empty abstracts, but not titles\n",
    "    \n",
    "    dataset['unq_id'] = dataset.index.values.tolist()\n",
    "    \n",
    "    dataset['Inclusion'] = pd.to_numeric(dataset['Inclusion'], errors = 'coerce' )\n",
    "    \n",
    "    dataset = dataset[~dataset['Abstract'].isnull()]\n",
    "    dataset = dataset[~dataset['Inclusion'].isnull()]\n",
    "\n",
    "    # fill with white space so joining doesn't go haywire\n",
    "\n",
    "    dataset = dataset.fillna(' ')\n",
    "    \n",
    "    #print(dataset.isna().sum())\n",
    "    dataset = dataset.replace(r'\\\\r',' ', regex = True)\n",
    "    dataset = dataset.replace(r'\\\\t',' ', regex = True)\n",
    "\n",
    "    dataset['All_Text'] = dataset.agg('{0[Title]} {0[Abstract]}'.format, axis=1)\n",
    "    \n",
    "    dataset['Metadata'] = dataset.agg('{0[Authors]} {0[Published.Year]} {0[Journal]} {0[Notes]}'.format, axis=1)\n",
    "    \n",
    "    print('\\tAfter ' + str(dataset.shape[0]))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# TODO: Do we want to remove numbers and special characters (e.g., other languages??)\n",
    "# Credit to Chantal\n",
    "def clean_text(s):\n",
    "    s = s.str.lower()                         # put to lowercase for homogeneity    \n",
    "    s = s.str.replace(r'_', ' ')              # remove underscores from the notes\n",
    "    s = s.str.replace(r'\\W', ' ')             # remove punctutation\n",
    "    stop = set(stopwords.words('english'))    # define stop words\n",
    "    lemmatizer = WordNetLemmatizer()          # lemmatize - a lot of repeat words\n",
    "    s = s.apply(lambda x: [lemmatizer.lemmatize(word, 'v')\n",
    "                              for word in x.split() \n",
    "                              if word not in stop]) # remove stopwords\n",
    "\n",
    "    s = s.apply(lambda x: [word for word in x if len(word) > 1])\n",
    "    s = s.apply(lambda x: [word for word in x if not word.isnumeric()])\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/delvin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/delvin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# need to only download only once\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = {}\n",
    "# see if output already exists, otherwise set to 0\n",
    "check = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/datasets_complete/sample_wash_data.csv\n",
      "Reading in data/datasets_complete/sample_wash_data.csv....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# assuming naming follows 'type' + '_complete.csv' structure \n",
    "\n",
    "for f in glob(os.path.join(data_dir, '*')):\n",
    "    \n",
    "    print(f)\n",
    "    \n",
    "    key = re.split(r'_', os.path.basename(f))[0]\n",
    "    \n",
    "    out_fn = os.path.join(output_dir, key + '_oct.tsv')\n",
    "    \n",
    "    if check == 1 and os.path.exists(out_fn): \n",
    "        print(f'Output file already exists for {key}, skipping!')\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        print(f'Reading in {f}....')\n",
    "        if f.endswith('csv'):\n",
    "            reviews[key] = pd.read_csv(f, encoding='latin1')#.fillna(' ')\n",
    "        elif f.endswith('xlsx'):\n",
    "            reviews[key] = pd.read_excel(f, encoding='latin1')#.fillna(' ')\n",
    "\n",
    "        \n",
    "# reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covidence..', 'Title', 'Authors', 'Abstract', 'Published.Year', 'Published.Month', 'Journal', 'Volume', 'Issue', 'Pages', 'Accession.Number', 'DOI', 'Ref', 'Study', 'Notes', 'Tags', 'Inclusion', 'FullText_Inclusion']\n"
     ]
    }
   ],
   "source": [
    "for key, dataset in reviews.items():\n",
    "    print(dataset.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Normalizing column names where needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covidence..', 'Title', 'Authors', 'Abstract', 'Published.Year', 'Journal', 'Notes', 'Inclusion', 'FullText_Inclusion']\n"
     ]
    }
   ],
   "source": [
    "to_keep = ['Title', 'Abstract', 'Notes','Published.Year', 'Covidence..', 'Inclusion', 'FullText_Inclusion',\n",
    "           'Authors', 'Journal']\n",
    "\n",
    "for key, dataset in reviews.items():\n",
    "    \n",
    "    if 'Published Year' in dataset.columns.tolist():\n",
    "        dataset.rename(columns = {'Published Year':'Published.Year'}, \n",
    "                 inplace=True)\n",
    "        \n",
    "    if 'Covidence #' in dataset.columns.tolist():\n",
    "        dataset.rename(columns = {'Covidence #':'Covidence..'}, \n",
    "                 inplace=True)\n",
    "        \n",
    "    filter_col = [col for col in dataset if col in to_keep]\n",
    "    \n",
    "    reviews[key] = dataset[filter_col]\n",
    "    print(reviews[key].columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the Data\n",
    "\n",
    "Purely data pre-processing here, ie. not NLP pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add a unique column identifier (`unq_id`) that can be created from the original data by taking the index along with other pre-processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "\tBefore 200\n",
      "\tAfter 200\n"
     ]
    }
   ],
   "source": [
    "for key, dataset in reviews.items():\n",
    "    # drop empty abstracts before concatenating them \n",
    "    print(key)\n",
    "    reviews[key] = preprocess_df(reviews[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From Chantal: \n",
    "Clean up and preprocess text: remove special characters, punctuation, tokenize, lemmatize, remove any repeated information (e.g., headings), replace NaNs with 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n"
     ]
    }
   ],
   "source": [
    "for key, dataset in reviews.items():\n",
    "    print(key)\n",
    "    dataset[['All_Text_Clean']] = dataset[['All_Text']].apply(lambda x: clean_text(x))\n",
    "    dataset['All_Text_Clean'] = dataset['All_Text_Clean'].str.join(' ')\n",
    "    \n",
    "    dataset[['Metadata_Clean']] = dataset[['Metadata']].apply(lambda x: clean_text(x))\n",
    "    dataset['Metadata_Clean'] = dataset['Metadata_Clean'].str.join(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save down the resulting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../cleaned_data/sample_oct.tsv successfully saved!\n"
     ]
    }
   ],
   "source": [
    "# save relevant columns\n",
    "\n",
    "    \n",
    "for key,dataset in reviews.items():\n",
    "    out_fn = os.path.join(output_dir, key + '_oct.tsv')\n",
    "    \n",
    "    #if not os.path.isfile(fn):\n",
    "    if 'FullText_Inclusion' not in dataset.columns.tolist():\n",
    "        dataset[['unq_id', 'All_Text_Clean', 'Metadata_Clean', 'Inclusion','Covidence..',\n",
    "                 'Title', 'Abstract', 'All_Text', 'Metadata']].to_csv(out_fn, index = False, sep = '\\t', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    else: \n",
    "        dataset[['unq_id', 'All_Text_Clean', 'Metadata_Clean', 'Inclusion', 'FullText_Inclusion', 'Covidence..',\n",
    "                 'Title', 'Abstract', 'All_Text', 'Metadata']].to_csv(out_fn, index = False, sep = '\\t', quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "    print('{} successfully saved!'.format(out_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
